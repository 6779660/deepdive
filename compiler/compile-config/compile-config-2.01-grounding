#!/usr/bin/env jq
# compile-config-2.01-grounding -- Adds processes for grounding the factor graph
##

include "constants";
include "util";
include "sql";

# skip adding grounding processes unless there are variables and factors defined
if (.deepdive_.schema.variables_  | length) == 0
or (.deepdive_.inference.factors_ | length) == 0
then . else

def factorWeightDescriptionSqlExpr:
    # to serialize weight parameters describing each weight
    [ ("\(.factorName)-" | asSqlLiteral)
    , (.weight_.params[] |
        "CASE WHEN \(asSqlIdent) IS NULL THEN ''
              ELSE \(asSqlIdent) || ''  -- XXX CAST(... AS TEXT) unsupported by MySQL
          END"
      )
    ] | join(" ||\("-" | asSqlLiteral)|| ")
    ;

.deepdive_ as $deepdive

###############################################################################

## variable/*/materialize
# Each internal variable table holding distinct variables should be
# materialized first for correct id assignment, etc.
| .deepdive_.execution.processes += merge($deepdive.schema.variables_[] | {
    "process/grounding/variable/\(.variableName)/materialize": {
        dependencies_: [
            "process/grounding/from_grounding",
            "data/\(.variablesTable)"
        ],
        style: "cmd_extractor", cmd: "
        : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}
        varPath=\"$DEEPDIVE_GROUNDING_DIR\"/variable/\(.variableName | @sh)
        mkdir -p \"$varPath\"
        cd \"$varPath\"

        deepdive db analyze \(.variablesTable | @sh)

        \(if .variableType == "categorical" then "
        # generate a table holding all distinct categories for categorical variables
        # so a unique id can be assigned to each of them
        deepdive create table \(.variablesCategoriesTable | @sh) as \(
        { SELECT:
            [ (.variablesCategoryColumns[]
            | { alias: "_\(.)", table: "v", column: . })
            , { alias: "count", expr: "COUNT(1)"             }
            , { alias: "cid"  , expr: "CAST(NULL AS BIGINT)" }
            ]
        , FROM: [ { alias: "v", table: .variablesTable } ]
        , GROUP_BY: [ .variablesCategoryColumns[] |         { table: "v", column: . }   ]
        } | asSql | asPrettySqlArg)
        # assign unique id to every distinct category
        deepdive db assign_sequential_id \(.variablesCategoriesTable | @sh) cid 0 1 \(
            # order by category columns to keep the cids and wids in sync
            [ .variablesCategoryColumns[]
            | { column: "_\(.)" }
            | asSqlExpr
            ] | join(", ") | @sh)

        deepdive db analyze \(.variablesCategoriesTable | @sh)

        " else "" end)

        # generate a table holding all distinct variables identified by @key columns
        # so a unique id can be assigned to each of them
        deepdive create table \(.variablesIdsTable | @sh) as \(
        { SELECT:
            [ (.variablesKeyColumns[]
            | { alias: ., table: "v", column: . }) # XXX we don't prefix user's columns (with _ like others) since they directly use this table and it can cause confusion
            , { alias: deepdiveVariableInternalLabelColumn, expr:
                # boolean variables just take one label, so aggregate with AND
                ( if .variableType == "boolean" then "EVERY(\"v\".\(.variablesLabelColumn | asSqlIdent))"
                # categorical variables should point to the category id that has a true label column
                else "MIN(CASE WHEN \"v\".\(.variablesLabelColumn | asSqlIdent) THEN \"c\".\"cid\" ELSE NULL END)"
                end) }
            , { alias: deepdiveVariableInternalFrequencyColumn, expr: "COUNT(1)" }
            , { alias: deepdiveVariableIdColumn, expr: "CAST(NULL AS BIGINT)" }
            ]
        , FROM: [ { alias: "v", table: .variablesTable } ]
        , JOIN:
            ( if .variableType == "boolean" then null else
                # categorical variables need a join with the categories table
                # to find the correct internal label, etc.
                [ { INNER: { alias: "c", table: .variablesCategoriesTable }
                  , ON: { and: [ .variablesCategoryColumns[]
                               | { eq: [ { table: "c", column: "_\(.)" }
                                       , { table: "v", column: . } ]
                                 } ] }
                  } ]
            end)
        , GROUP_BY: [ .variablesKeyColumns[] | { column: . } ]
        } | asSql | asPrettySqlArg)

        deepdive db analyze \(.variablesIdsTable | @sh)
        "
    }
})

## variable_id_partition
# Grounding begins by counting the variables to partition a range of
# non-negative integers for assigning the variable ids.
| .deepdive_.execution.processes += {
    "process/grounding/variable_id_partition": {
        dependencies_: [
            # id partition depends on all distinct variables to be first identified
            $deepdive.schema.variables_[] | "process/grounding/variable/\(.variableName)/materialize"
        ],
        style: "cmd_extractor", cmd: "
        : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}

        RANGE_BEGIN=0 \\
        partition_id_range \($deepdive.schema.variables_ | map(.variablesIdsTable | @sh) | join(" ")) | {
            # variable names
            set -- \($deepdive.schema.variables_ | map(.variableName | @sh) | join(" "))
            # record the base
            variableCountTotal=0
            while read table begin excludeEnd; do
                varName=$1; shift
                varPath=\"$DEEPDIVE_GROUNDING_DIR\"/variable/${varName}
                mkdir -p \"$varPath\"
                cd \"$varPath\"
                echo $begin                      >id_begin
                echo $excludeEnd                 >id_exclude_end
                echo $(( $excludeEnd - $begin )) >count
                variableCountTotal=$excludeEnd
            done
            # record the final count
            echo $variableCountTotal >\"$DEEPDIVE_GROUNDING_DIR\"/variable_count
        }
        "
    }
}


## variable/*/assign_id
# Each variable table then gets the range of integers assigned to the id column
# of every row.
| .deepdive_.execution.processes += merge($deepdive.schema.variables_[] | {
    "process/grounding/variable/\(.variableName)/assign_id": {
        dependencies_: [
            "process/grounding/variable_id_partition"
        ],
        style: "cmd_extractor", cmd: "
        : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}

        cd \"$DEEPDIVE_GROUNDING_DIR\"/variable/\(.variableName | @sh)
        baseId=$(cat id_begin)

        # assign id to all distinct variables according to the paritition
        deepdive db assign_sequential_id \(.variablesIdsTable | @sh) \(deepdiveVariableIdColumn | @sh) $baseId
        "
    }
})

## variable_holdout
# Variables to holdout are recorded by executing either a user-defined
# (app-wide) holdout query, or by taking a random sample of a user-defined
# fraction.
# TODO easier way to do holdout per variable?
| .deepdive_.execution.processes += {
    "process/grounding/variable_holdout": {
        dependencies_: [
            $deepdive.schema.variables_[]
            | "process/grounding/variable/\(.variableName)/assign_id"
        ],
        style: "cmd_extractor", cmd: "
        : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}

        deepdive create table \(deepdiveGlobalHoldoutTable | @sh) \\
            variable_id:BIGINT:'PRIMARY KEY' \\
            #
        deepdive create table \(deepdiveGlobalObservationTable | @sh) \\
            variable_id:BIGINT:'PRIMARY KEY' \\
            #
        \([ if $deepdive.calibration.holdout_query then
            # run user holdout query if configured
            "\($deepdive.calibration.holdout_query)"
          else
            # otherwise, randomly select from evidence variables of each variable table
            $deepdive.schema.variables_[] | "
                INSERT INTO \(deepdiveGlobalHoldoutTable | asSqlIdent) \(
                { SELECT: [ { column: deepdiveVariableIdColumn } ]
                , FROM: [ { table: .variablesIdsTable } ]
                , WHERE:
                    [ { isntNull: { column: deepdiveVariableInternalLabelColumn } }
                    , { lt: [ { expr: "RANDOM()" }
                            , { expr: $deepdive.calibration.holdout_fraction }
                            ]
                      }
                    ]
                } | asSql);
            "
          end
        , if $deepdive.calibration.observation_query then
            # run user holdout query if configured
            "\($deepdive.calibration.holdout_query)"
          else empty
          end
        ] | map("deepdive sql \(asPrettySqlArg)") | join("\n"))
        "
    }
}

## variable/*/dump
# Then each variable table is dumped into a set of binary files for the inference engine.
| .deepdive_.execution.processes += merge($deepdive.schema.variables_[] | {
    "process/grounding/variable/\(.variableName)/dump": {
        dependencies_: [
            "process/grounding/variable_holdout"
          # XXX below can be omitted for now
          #, "process/grounding/variable/\(.variableName)/assign_id"
        ],
        style: "cmd_extractor", cmd: "
        : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}

        varPath=\"$DEEPDIVE_GROUNDING_DIR\"/variable/\(.variableName | @sh)
        mkdir -p \"$varPath\"
        cd \"$varPath\"
        find . -name 'variables.part-*.bin.bz2' -exec rm -rf {} +
        export DEEPDIVE_LOAD_FORMAT=tsv
        export DEEPDIVE_UNLOAD_MATERIALIZED=false

        # dump the variables, joining the holdout query to determine the type of each variable
        deepdive compute execute \\
            input_sql=\(
            { SELECT:
                [ { column: "vid" }
                , { column: "variable_role" }
                , { alias: "init_value", expr:
                    "CASE WHEN variable_role = 0 THEN 0
                          ELSE (\(
                            if   .variableType == "boolean"     then "CASE WHEN label THEN 1 ELSE 0 END"
                            elif .variableType == "categorical" then "label"
                            else error("Internal error: Unknown variableType: \(.variableType)")
                            end
                            ))
                      END" }
                , { column: "variable_type" }
                , { column: "cardinality" }
                ]
            , FROM: { alias: "variables", sql:
                { SELECT:
                    [ { alias: "vid", table: "i", column: deepdiveVariableIdColumn }
                    , { alias: "variable_role", expr:
                          "CASE WHEN observation.variable_id IS NOT NULL
                                 AND \"i\".\(deepdiveVariableInternalLabelColumn | asSqlIdent) IS NOT NULL THEN 2
                                WHEN holdout.variable_id IS NOT NULL THEN 0
                                WHEN \"i\".\(deepdiveVariableInternalLabelColumn | asSqlIdent) IS NOT NULL THEN 1
                                ELSE 0
                            END" }
                    , { alias: "label", table: "i", column: deepdiveVariableInternalLabelColumn }
                    , { alias: "variable_type", expr:
                            ( if .variableType == "boolean"     then 0
                            elif .variableType == "categorical" then 1
                            else error("Internal error: Unknown variableType: \(.variableType)")
                            end) }
                    , { alias: "cardinality", expr:
                            ( if .variableType == "boolean"     then 2
                            elif .variableType == "categorical" then "\"i\".\"dd__count\"" # TODO count the number of actual distinct values by .variablesCategoryColumns for this row
                            else error("Internal error: Unknown variableType: \(.variableType)")
                            end) }
                    ]
                , FROM: { alias: "i", table: .variablesIdsTable }
                , JOIN:
                    [ { LEFT_OUTER: { alias: "holdout", table: deepdiveGlobalHoldoutTable }
                      , ON: { eq: [ { table: "i", column: deepdiveVariableIdColumn }
                                  , { table: "holdout"  , column: "variable_id" } ] } }
                    , { LEFT_OUTER: { alias: "observation", table: deepdiveGlobalObservationTable }
                      , ON: { eq: [ { table: "i"  , column: deepdiveVariableIdColumn }
                                  , { table: "observation", column: "variable_id" } ] } }
                    ]
                } }
            } | asSql | asPrettySqlArg) \\
            command=\("
                sampler-dw text2bin variable /dev/stdin >(pbzip2 >variables.part-${DEEPDIVE_CURRENT_PROCESS_INDEX}.bin.bz2)
            " | @sh) \\
            output_relation=
        "
    }

})


## variable/*/dump_domains
# Each categorical variable dumps an extra input for the inference engine that holds the domains.
| .deepdive_.execution.processes += merge($deepdive.schema.variables_[]
    | select(.variableType == "categorical") | {
    "process/grounding/variable/\(.variableName)/dump_domains": {
        dependencies_: [
            "process/grounding/variable/\(.variableName)/assign_id"
        ],
        style: "cmd_extractor", cmd: "
        : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}
        varPath=\"$DEEPDIVE_GROUNDING_DIR\"/variable/\(.variableName | @sh)
        mkdir -p \"$varPath\"
        cd \"$varPath\"
        find . -name 'domains.part-*.bin.bz2' -exec rm -rf {} +
        export DEEPDIVE_LOAD_FORMAT=tsv
        export DEEPDIVE_UNLOAD_MATERIALIZED=false

        # dump the categorical variable domains, joining the categories table for their ids
        deepdive compute execute \\
        input_sql=\(
        # sparse categorical variables have domains files
        { SELECT:
            [ { alias: "vid",  table: "i", column: deepdiveVariableIdColumn }
            , { alias: "cardinality", expr: "COUNT(c.cid)" }
            , { alias: "cids", expr: "ARRAY_AGG(c.cid ORDER BY c.cid)" }
            ]
        , FROM: [ { alias: "v", table: .variablesTable } ]
        , JOIN:
            # variable ids
            [ { INNER: { alias: "i", table: .variablesIdsTable }
              , ON: { and:  [ .variablesKeyColumns[]
                            | { eq: [ { table: "i", column: . }
                                    , { table: "v", column: . }
                                    ] }
                            ] } }
            # category ids are necessary to find the inference result corresponding to the variable
            , { INNER: { alias: "c", table: .variablesCategoriesTable }
              , ON: { and:  [ .variablesCategoryColumns[]
                            | { eq: [ { table: "c", column: "_\(.)" }
                                    , { table: "v", column: . }
                                    ] }
                            ] } }
            ]
        , GROUP_BY: [ { table: "i", column: deepdiveVariableIdColumn } ]
        } | asSql | asPrettySqlArg) \\
        command=\("
            sampler-dw text2bin domain /dev/stdin >(pbzip2 >domains.part-${DEEPDIVE_CURRENT_PROCESS_INDEX}.bin.bz2)
        " | @sh) \\
        output_relation=
        "
    }

})


###############################################################################

## factor/*/materialize
# Each inference rule's SQL query is run to materialize the factors and the
# distinct weights used in them.
| .deepdive_.execution.processes += merge($deepdive.inference.factors_[]
    | [ .input_[]
      | ltrimstr("data/")
      | $deepdive.schema.variables_byName[.]
      | select(type != "null")
      ] as $schemaVariablesForThisFactor
    | {
    # add a process for grounding factors
    "process/grounding/factor/\(.factorName)/materialize": {
        # materializing each factor requires the dependent variables to have their id assigned
        dependencies_: [
            $schemaVariablesForThisFactor[]
            # the involved variables must have their ids all assigned
            | "process/grounding/variable/\(.variableName)/assign_id"?
        ],
        # other non-variable tables are also necessary
        input_: [ .input_[]
            | select(ltrimstr("data/") | in($deepdive.schema.variables_byName) | not)
        ],
        style: "cmd_extractor", cmd: "
            : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}
            facPath=\"$DEEPDIVE_GROUNDING_DIR\"/factor/\(.factorName | @sh)
            mkdir -p \"$facPath\"
            cd \"$facPath\"

            # materialize factors using user input_query that pulls in assigned ids to involved variables
            deepdive create table \(.factorsTable | @sh) as \(
                # Not using DISTINCT: User decides if they want to have duplicate factors
                { SELECT:
                    [ ( .function_.variables[] | { table: "F", column: .columnId } )
                    # TODO: assuming categorical factors' heads have categorical vars only
                    , ( .function_ | select(.isCategorical) | .variables[]
                      | .columnPrefix as $prefix
                      | .schema.variablesCategoryColumns[]
                      | { table: "F", column: "\($prefix)\(.)" } )
                    , ( .weight_.params[] | { table: "F", column: . } )
                    , { table: "F", column: "factor_value", alias: "factor_value" }
                    ]
                , FROM: [
                    { alias: "F", sql: "\(.input_query)" }
                    ]
                } | asSql | asPrettySqlArg)

            # find distinct weights for the factors into a separate table
            deepdive create table \(.weightsTable | @sh) as \(
                { SELECT:
                    # weight parameters
                    [ ( .weight_.params[] | { column: . } )
                    # weight attributes
                    , { alias: "isfixed"  , expr: .weight_.is_fixed      }
                    , { alias: "initvalue", expr: .weight_.init_value    }
                    , { alias: "wid"      , expr: "CAST(NULL AS BIGINT)" } # to be assigned later by the assign_weight_id process
                    ]
                , DISTINCT: true  # XXX this is inevitable
                , FROM:
                    [ select(.weight_.params | length > 0)
                    # when weight is parameterized, find all distinct ones
                    | { alias: "f", table: .factorsTable }
                    ]
                } | asSql | asPrettySqlArg)
        "
    }
})


## weight_id_partition
# The weight ids must be first partitioned by counting them.
| .deepdive_.execution.processes += {
    "process/grounding/weight_id_partition": {
        dependencies_: [
            $deepdive.inference.factors_[]
            | "process/grounding/factor/\(.factorName)/materialize"
        ],
        style: "cmd_extractor", cmd: "
        : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}
        cd \"$DEEPDIVE_GROUNDING_DIR\"/factor

        \([ "set --", "\($deepdive.inference.factors_[].weightsTable | @sh)", "#" ] | join(" \\\n    "))

        # partition the id range for weights
        RANGE_BEGIN=0 RANGE_STEP=1 \\
        partition_id_range \"$@\" | {
            # factor names
            set -- \($deepdive.inference.factors_ | map(.factorName | @sh) | join(" "))
            weightsCountTotal=0
            while read table begin excludeEnd; do
                factor=$1; shift
                facPath=\"$DEEPDIVE_GROUNDING_DIR\"/factor/${factor}
                mkdir -p \"$facPath\"
                cd \"$facPath\"
                echo $begin                      >weights_id_begin
                echo $excludeEnd                 >weights_id_exclude_end
                echo $(( $excludeEnd - $begin )) >weights_count
                weightsCountTotal=$excludeEnd
            done
            echo $weightsCountTotal >\"$DEEPDIVE_GROUNDING_DIR\"/factor/weights_count
        }
        "
    }
}

## factor/*/assign_weight_id
# Each inference rule gets its weight ids actually assigned.
| .deepdive_.execution.processes += merge($deepdive.inference.factors_[] | {
    "process/grounding/factor/\(.factorName)/assign_weight_id": {
        dependencies_: [
            "process/grounding/weight_id_partition"
        ],
        style: "cmd_extractor", cmd: "
            : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}

            cd \"$DEEPDIVE_GROUNDING_DIR\"/factor/\(.factorName | @sh)
            baseId=$(cat weights_id_begin)

            # assign weight ids according to the partition
            deepdive db assign_sequential_id \(.weightsTable | @sh) wid $baseId

            deepdive db analyze \(.weightsTable | @sh)
        "
    }
})

## global_weight_table
# To view the weights learned by the inference engine later, set up an app-wide table.
| .deepdive_.execution.processes += {
    "process/grounding/global_weight_table": {
        dependencies_: [
            $deepdive.inference.factors_[] |
                "process/grounding/factor/\(.factorName)/materialize"
        ],
        style: "cmd_extractor", cmd: "
        : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}

        # set up a union view for all weight tables (\(deepdiveGlobalWeightsTable | asSqlIdent))
        deepdive create view \(deepdiveGlobalWeightsTable | @sh) as \(
            [ $deepdive.inference.factors_[] |
                { SELECT:
                    [ { column: "wid" }
                    , { column: "isfixed" }
                    , { column: "initvalue" }
                    , { alias: "description", expr: factorWeightDescriptionSqlExpr }
                    ]
                , FROM:
                    [ { table: .weightsTable }
                    ]
                } | asSql | "(\(.))"
            ] | join("\nUNION ALL\n") | asPrettySqlArg)
        "
    }
}

## factor/*/dump
# The factors are dumped into a set of binary files for the inference engine.
| .deepdive_.execution.processes += merge($deepdive.inference.factors_[] | {
    # add a process for grounding factors and weights
    "process/grounding/factor/\(.factorName)/dump": {
        dependencies_: [
            "process/grounding/factor/\(.factorName)/assign_weight_id"
        ],
        style: "cmd_extractor", cmd: "
            : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}
            facPath=\"$DEEPDIVE_GROUNDING_DIR\"/factor/\(.factorName | @sh)
            mkdir -p \"$facPath\"
            cd \"$facPath\"
            find . \\( -name  'factors.part-*.bin.bz2' \\
                    -o -name 'nfactors.part-*'         \\
                    -o -name   'nedges.part-*'         \\
                   \\) -exec rm -rf {} +
            export DEEPDIVE_LOAD_FORMAT=tsv
            export DEEPDIVE_UNLOAD_MATERIALIZED=false

            # named codes used by sampler
            export \(.function_.name)Factor=\(.function_.id)  # factor function id
            export numVariablesForFactor=\(.function_.variables | length)
            export areVariablesPositive=\(.function_.variables | map(if .isNegated then "0" else "1" end) | join(" ") | @sh)

            # dump the factors joining the assigned weight ids, converting into binary format for the inference engine
            deepdive compute execute \\
                input_sql=\(
                if .function_.isCategorical then
                "\"$(
                    # categorical factor need to find exact combinations of category values and
                    # weight parameters present in the data, hence a lot of joins! but doesn't waste the weights
                    # Assuming all var tables are categorical; TODO: Support mixed head.
                    echo \(
                        # Order for parallel ARRAY_AGG expressions
                        ( [ .function_.variables[]
                          | { table: "C\(.ordinal)", column: "cid" } | asSqlExpr
                          ] | join(", ")) as $valueOrder
                    |
                        .non_category_weight_cols as $non_category_weight_cols
                    |
                    { SELECT:
                        [ ( .function_.variables[]
                        | { table: "f", column: .columnId } )
                        , { alias: "num_weights", expr: "COUNT(w.wid)" }
                        # arrays below are parallel (ensured by $valueOrder)
                        , ( .function_.variables[]
                        | { alias: "c\(.ordinal)_ids",
                            expr: "ARRAY_AGG(\({ table: "C\(.ordinal)", column: "cid"
                                               } | asSqlExpr) ORDER BY \($valueOrder))" } )
                        , { alias: "weight_ids" , expr: "ARRAY_AGG(w.wid ORDER BY \($valueOrder))" }
                        , { alias: "factor_values" , expr: "ARRAY_AGG(f.factor_value ORDER BY \($valueOrder))" }
                        ]
                    , FROM:
                        [ { alias: "w", table: .weightsTable }
                        , { alias: "f", table: .factorsTable }
                        ]
                    , JOIN: [(.function_.variables[]
                        | "C\(.ordinal)" as $ctable
                        | .columnPrefix as $prefix
                        | { INNER: { alias: $ctable, table: .schema.variablesCategoriesTable }
                          , ON: { and: [ .schema.variablesCategoryColumns[]
                                 | { eq: [ { table: $ctable, column: "_\(.)" }
                                         , { table: "f", column: "\($prefix)\(.)" }
                                         ] }
                                 ] } }
                        )]
                    , WHERE:
                        [ ( .weight_.params[]
                        | { eq: [ { table: "w", column: . }
                                , { table: "f", column: . } ] } )
                        ]
                    , GROUP_BY:
                        [ ( .function_.variables[] | { table: "f", column: .columnId } ),
                          ( $non_category_weight_cols[] | { table: "f", column: . } ) ]
                    } | asSql | asPrettySqlArg)
                )\"" else # factors over boolean variables
                    { SELECT:
                        [ ( .function_.variables[]
                        | { table: "f", column: .columnId } )
                        , { alias: "weight_id", table: "w", column: "wid" }
                        , { alias: "factor_value", table: "f", column: "factor_value" }
                        ]
                    , FROM:
                        [ { alias: "f", table: .factorsTable }
                        , { alias: "w", table: .weightsTable }
                        ]
                    , WHERE:
                        [ ( .weight_.params[]
                        | { eq: [ { table: "w", column: . }
                                , { table: "f", column: . } ] } )
                        ]
                    } | asSql | asPrettySqlArg
                end) \\
                command=\("
                    # also record the factor count
                    tee >(wc -l >nfactors.part-${DEEPDIVE_CURRENT_PROCESS_INDEX}) |
                    sampler-dw text2bin factor /dev/stdin >(pbzip2 >factors.part-${DEEPDIVE_CURRENT_PROCESS_INDEX}.bin.bz2) \\
                        $\(.function_.name)Factor \\
                        $numVariablesForFactor \\
                        $areVariablesPositive |
                    # and the edge count
                    tee nedges.part-${DEEPDIVE_CURRENT_PROCESS_INDEX}
                " | @sh) \\
                output_relation=
        "
    }
})

## factor/*/dump_weights
# The factors and weights are dumped into a set of binary files for the inference engine.
| .deepdive_.execution.processes += merge($deepdive.inference.factors_[] | {
    # add a process for grounding factors and weights
    "process/grounding/factor/\(.factorName)/dump_weights": {
        dependencies_: [
            "process/grounding/factor/\(.factorName)/assign_weight_id"
        ],
        style: "cmd_extractor", cmd: "
            : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}
            facPath=\"$DEEPDIVE_GROUNDING_DIR\"/factor/\(.factorName | @sh)
            mkdir -p \"$facPath\"
            cd \"$facPath\"
            find . \\( -name  'weights.part-*.bin.bz2' \\
                   \\) -exec rm -rf {} +
            export DEEPDIVE_LOAD_FORMAT=tsv
            export DEEPDIVE_UNLOAD_MATERIALIZED=false

            # flag that signals whether to reuse weights or not
            reuseFlag=\"$DEEPDIVE_GROUNDING_DIR\"/factor/weights.reuse

            # dump the weights (except the description column), converting into binary format for the inference engine
            deepdive compute execute \\
                input_sql=\"$(if [[ -e \"$reuseFlag\" ]]; then
                    echo \(
                    # dump weights with initvalue from previously learned ones
                    { SELECT:
                        [ { table: "w", column: "wid" }
                        , { expr: "CASE WHEN w.isfixed THEN 1 ELSE 0 END" }
                        , { expr: "COALESCE(reuse.weight, w.initvalue, 0)" }
                        ]
                    , FROM: [ { alias: "w", table: .weightsTable } ]
                    , JOIN: { LEFT_OUTER: { alias: "reuse", table: deepdiveReuseWeightsTable }
                            , ON: { eq: [ { table: "reuse", column: "description" }
                                        , { expr: factorWeightDescriptionSqlExpr }
                                        ] }
                            }
                    } | asSql | asPrettySqlArg)
                else
                    echo \(
                    # dump weights from scratch
                    { SELECT:
                        [ { column: "wid" }
                        , { expr: "CASE WHEN isfixed THEN 1 ELSE 0 END" }
                        , { expr: "COALESCE(initvalue, 0)" }
                        ]
                    , FROM: [ { table: .weightsTable } ]
                    } | asSql | asPrettySqlArg)
                fi)\" \\
                command=\("
                    sampler-dw text2bin weight /dev/stdin >(pbzip2 >weights.part-${DEEPDIVE_CURRENT_PROCESS_INDEX}.bin.bz2)
                " | @sh) \\
                output_relation=
        "
    }
})

###############################################################################

# Finally, put together everything dumped into a layout the inference engine can easily load from
| .deepdive_.execution.processes += {
    "process/grounding/combine_factorgraph": {
        dependencies_: [(
            $deepdive.schema.variables_[]
            | "process/grounding/variable/\(.variableName)/dump"
            , (select(.variableType == "categorical")
            | "process/grounding/variable/\(.variableName)/dump_domains")
        ), (
            $deepdive.inference.factors_[]
            | "process/grounding/factor/\(.factorName)/dump"
            , "process/grounding/factor/\(.factorName)/dump_weights"
        ), (
            "process/grounding/global_weight_table"
        )],
        output_: ["model/factorgraph"],
        style: "cmd_extractor", cmd: (
            ([$deepdive.schema.variables_[] | .variableName | @sh] | join(" ")) as $variableNames |
            ([$deepdive.inference.factors_[] | .factorName  | @sh] | join(" ")) as $factorNames   |
        "
        : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}
        : ${DEEPDIVE_FACTORGRAPH_DIR:=\"$DEEPDIVE_APP\"/run/model/factorgraph}

        # create a fresh empty directory for the new combined factor graph
        rm -rf   \"$DEEPDIVE_FACTORGRAPH_DIR\"
        mkdir -p \"$DEEPDIVE_FACTORGRAPH_DIR\"
        cd \"$DEEPDIVE_FACTORGRAPH_DIR\"

        # create symlinks to the grounded binaries by enumerating variables and factors
        for v in \($variableNames); do
            mkdir -p {variables,domains}/\"$v\"
            find \"$DEEPDIVE_GROUNDING_DIR\"/variable/\"$v\" \\
                -name 'variables.part-*.bin.bz2' -exec ln -sfnv -t variables/\"$v\"/ {} + \\
                -o \\
                -name   'domains.part-*.bin.bz2' -exec ln -sfnv -t   domains/\"$v\"/ {} + \\
                #
        done
        for f in \($factorNames); do
            mkdir -p {factors,weights}/\"$f\"
            find \"$DEEPDIVE_GROUNDING_DIR\"/factor/\"$f\" \\
                -name 'factors.part-*.bin.bz2' -exec ln -sfnv -t factors/\"$f\"/ {} + \\
                -o \\
                -name 'weights.part-*.bin.bz2' -exec ln -sfnv -t weights/\"$f\"/ {} + \\
                #
        done

        # generate the metadata for the inference engine
        {
            # first line with counts of variables and edges in the grounded factor graph
            cd \"$DEEPDIVE_GROUNDING_DIR\"
            sumup() { { tr '\\n' +; echo 0; } | bc; }
            counts=()
            counts+=($(cat factor/weights_count))
            # sum up the number of factors and edges
            counts+=($(cat variable_count))
            cd factor
            counts+=($(find \($factorNames) -name 'nfactors.part-*' -exec cat {} + | sumup))
            counts+=($(find \($factorNames) -name 'nedges.part-*'   -exec cat {} + | sumup))
            (IFS=,; echo \"${counts[*]}\")
            # second line with file paths
            paths=(\"$DEEPDIVE_FACTORGRAPH_DIR\"/{weights,variables,factors,edges,domains})
            (IFS=,; echo \"${paths[*]}\")
        } >meta
        ")
    }
}

## from_grounding
# A nominal process to make it easy to redo the grounding
# TODO remove this once deepdive-do supports process groups or pipelines
| .deepdive_.execution.processes += {
    "process/grounding/from_grounding": {
        style: "cmd_extractor", cmd: ": no-op"
    }
}

end
